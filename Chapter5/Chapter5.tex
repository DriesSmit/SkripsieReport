\chapter{Analysis of results}
\label{ch:Results}
\ifpdf
\graphicspath{{Chapter5/Chapter5Figures/}}
\fi

In the following chapter a study is described on the accuracy of the test grading system. In the first two sections of this chapter a basic system and complete system is tested using the same datasets. This allows for a accurate interpretation and comparison between the two systems. The first test grading system is implemented using only the image processing techniques, described in Chapter \ref{cp:ImageProcessing}. The second system is the final complete system implemented in this project and includes all the machine learning techniques described in Chapter \ref{ch:MachineLearning}. In the last section a description of 3 additional validation tests is given. These tests are done on tutorials written by Applied Mathematics students as is described in the agreement with the department.

Each systems is assessed in the next section under 3 categories. Firstly some marking statistics will be given on the tests grade. The second category describes all the tests, the system appointed to be marked manually, because it was not sure what the correct answer is. In this catagory the tests gets sent to a clashlist. An image, with and interface to input the correct values gets displayed. The user must then look at the image and enter the correct answer. Lastly, all the answers that the system did not assign for manual marking, but identified incorrectly will be described.

\section{Results of 25 test cases}

In this section the results of grading 25 hand picked tests is compared between the two systems. Among the 25 tests there are different categories that tests different aspects and limitations of the systems. The categories and the number of case included can be seen in the list below:

\begin{enumerate}
\item Test with crossed out answers(7).
\item Test with lightly coloured entries or partially coloured in entries(4).
\item Test with negative signed answers(1, but also included under other categories as well).
\item Test with no bubbles and only characters filled in for a specific entry field(5)
\item Test with no characters filled in and only bubbles for a specific entry field(2)
\item Test with data filled in correctly(2).
\item Random page with no template on them(2).
\item Page with tilted template inside image(2).
\end{enumerate}

These test is specifically chosen, because in combination it approximates all the types of tests the system has to asses. Most of the tests are slightly extreme cases of what students has filled in on tutorial tests. 

\subsection{Basic system}


\subsubsection{Marking statistics}

Using this basic system an average time for each test is calculated to be 0.305 seconds. 

\begin{figure}
  \centering
  \includegraphics[width=8cm]{crossClash}\\
  \caption{Image showing answer with crossed out answers that the system misinterpreted.}
  \label{fig:crossClash}
\end{figure}

\subsubsection{Clashlist}

A total of 6 out of the 25 test where reported as clashes. The two random images were both reported in the clash list. Further the one test which only has the student number written in with no bubbles filled in is also reported to the list. The other 3 clashes were reported due to the crossed out bubbles interpreted as still filled in, causing the system to think that two answer were filled in. An example of this can be seen in Figure \ref{fig:crossClash}.

\subsubsection{Incorrect automatic graded results}

There where 4 tests in total that where graded automatically, but wrongly. All of these test were tests that had only character information in atleast one of the answers. An example of this can be seen in Figure \ref{fig:OnlyCharacters}

\begin{figure}
  \centering
  \includegraphics[width=8cm]{OnlyCharacters}\\
  \caption{Student filled in answer with only character information.}
  \label{fig:OnlyCharacters}
\end{figure}

\subsection{Complete system}

\subsubsection{Marking statistics}

Using this complete system an average time for each test is calculated to be 2.011 seconds. 

\subsubsection{Clashlist}

A total of 6 out of the 25 test where reported as clashes. The two random images were both reported in the clash list. One test was reported due to it only having characters written in with no bubbles. Thus even though the system identified every character correctly, it had to low of a percentage faith in its answer and reported it to the clash list. The final two cases that was reported to the clash list was due to the character recognition determining a crossed out character as the intended character. An example of this is shown in Figure \ref{fig:crossedOutCharacter}.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{crossedOutCharacter}\\
  \caption{Crossed out character that confused the grading system.}
  \label{fig:crossedOutCharacter}
\end{figure}

\subsubsection{Incorrect automatic graded results}

There were no automatically graded answers that was done incorrectly.

\subsection{Analysis or results}

As is seen in the section below, the complete systems does not reduce the number of clashes for these 25 images. The complete system sometimes classifies a crossed out character is the intended character in that block. But because it is crossed out the system assigns a low certainty to that value, as the neural network is not to sure which digit it is. A drawback of the completed system is that it takes on average 2.011 seconds to grade a tests. This is due to the student number PGM taking on average 1.5 seconds to infer the correct student number. The complete system had no incorrectly automatically graded answers in contras to the 4 graded wrongly using the basic system. A reason to this is attributed to more evidence that gets considered in the complete system. Thus only if that evidence match up will the system be certain enough to accept its answer as the correct one. In the next section the complete system use used to grade a tutorial written by students. 

\section{Grading of tutorial tests}

In this section the complete system is tested on the final tutorial test written by the student class. Thus the final version of the complete system was used in grading the students' tests. To analyse the results student feedback was recorded to locate tests that were graded incorrectly. 

\subsection{Marking statistics}

For these tutorial test  an average marking time per test of 2.3 seconds was recorded.

\subsection{Clashlist}

In total there were 67 clashes in the 888 tests. These 67 clashes are categorised in Table  \ref{tbl:TutClash}:
\begin{table}
  \centering
\begin{tabular}{|c|c|}
\hline
\textbf{Number of tests in category} & \textbf{Category description}\\
\hline
41&In these tests the system guessed the right values,\\ 
&but with a certainty value below 90\%. Some of the\\
&cases was when the student number was only filled\\
&in the character box. The software always\\
&identified the correct student number, but was\\
&below 90\% sure of its overall test answer.\\
\hline
15&In these tests the system could not distinguish\\ 
&between a crossed out answer and correct answer.\\
&This is due to the crossed out answer being\\
&interpreted as a filled in answer.\\
\hline
8&These tests have an answer with only character\\ 
&information in them. The system tried to identify\\
&each answer, but made a mistake in atleast one of the\\
&digits.\\
\hline
2&These images contained blank papers that did not\\ 
&include test templates.\\
\hline
1&In this test the grid of the test paper can not be\\ 
&found. Thus the test could not be marked.\\
\hline
\end{tabular}
  \caption{Table describing number of clashes in the different catagories.} \label{tbl:TutClash}
\end{table}

\subsection{Incorrect automatic graded results}

For this tutorial 888 test were written. To check through every test manually to find mistakes becomes impractical. Thus the students were asked to report any results that were marked wrong by the system. This is all the results that the system decided to automatically grade and in doing so graded the test wrong.

Only 1 result was reported where the software automatically marked the wrong answer to a test. The correct answer was -95.0 and the system marked the answer as 95.0, as seen in Figure \ref{fig:wrongAns}. There might still be test that were marked wrong, but these test(s) were not reported.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{wrongResult}\\
  \caption{Incorrectly identified answer as 95.}
  \label{fig:wrongAns}
\end{figure}

For a more detailed description of the results from grading all 4 tutorial tests, refer to Appendix \ref{ap:results}.

\subsection{Analysis or results}

In conclusion it is noted that the complete system, with its machine learning capabilities, slightly reduces the number of tests that the user must mark manually from the previous basic system. About 7.5\% of the tests in the tutorial had to be marked manually, due to the system being unsure of its answers. The system does however have a high probability of grading tests correctly if they are done automatically. A reason for this can be attributed to the fact that the system correlates two pieces of evidence to predict the correct answer. Thus both the character and bubble information has to be interpreted incorrectly for the system to automatically grade a answer incorrectly. 

Thus overall the system could graded 92.5\% of all the test correctly and automatically. From the remaining 7.5\% only 1 test or 0.1\% of the tests was found to be graded wrongly. For the remaining 7.4\% of the tests the system was not certain enough to grade these test automatically. The system did classify most of the clashlist test correctly. This means the system could possible have graded 97.1\% of the tests correctly. This can be done by changing a threshold value, but allows the other 2.9\% of the tests to be classified incorrectly.

