\chapter{Validation and results}
\label{ap:results}
\graphicspath{{Appendix5/Appendix5figures/}}

This appendix described additional test results obtained from experiments done on the automatic test grading systems.

\section{All tutorial results}
\label{sec:tutorialResults}

\subsection{Overview}

This automatic test grader was successfully used to grade 11 tutorial test in 2017. The amount of tests per tutorial varies due to students having valid excuses. On average 889 tests were written per tutorial. The results of these tutorials can be seen in Table \ref{tbl:tutResults} and Table \ref{tbl:tutResults2}.

\begin{table}
\caption{Description of tutorial results.} \label{tbl:tutResults}
  \centering
\begin{tabular}{|p{2cm}|p{4cm}|p{5cm}|}
\hline
\textbf{Tutorial number}&\textbf{Percentage tests graded correctly}&\textbf{Reason for results}\\
\hline
\multicolumn{3}{|l|}{Basic system is now implemented.}\\
\hline
Tutorial 1&98.4\% (14 mistakes)&The system had problems with identifying crossed out answers.\\
\hline
Tutorial 2&98.8\% (11 mistakes)&The system still had a problem with crossed out answers.  This problem was subsequently resolved.\\
\hline
Tutorial 3&99.4\% (5 mistakes)&The system made a few mistakes with answers with only character information.\\
\hline
Tutorial 4&98.5\% (13 mistakes)&A rounding mistake in the software led to some answers being marked incorrectly.\\
\hline
Tutorial 5&99.3\% (5 mistakes)&The system made a few mistakes with answers with only character information.\\
\hline
Tutorial 6&99.7\% (3 mistakes)&The system made a few mistakes with answers with only character information.\\
\hline
\end{tabular} 
\end{table}

\begin{table}
\caption{Description of tutorial results.} \label{tbl:tutResults2}
  \centering
\begin{tabular}{|p{2cm}|p{4cm}|p{5cm}|}
\hline
\textbf{Tutorial number}&\textbf{Number of tests graded incorrectly}&\textbf{Reason for results}\\
\hline
\multicolumn{3}{|l|}{Compete system is now implemented.}\\
\hline
Tutorial 7&99.9\% (1 mistake)&The system classified a crossed out answer as being coloured in.\\
\hline
Tutorial 8&99.8\% (2 mistakes)&The system classified a crossed out answer as being coloured in. This problem was subsequently resolved.\\
\hline
Tutorial 9&100.0\% (0 mistakes)&No mistakes where found.\\
\hline
Tutorial 10&99.9\% (1 mistakes)&This tutorial was discussed in the results chapter. The student wrote over the negative sign bubble, confusing the system. This mistake is attributed to the student.\\
\hline
Tutorial 11&100.0\% (0 mistakes)&No mistakes where found.\\
\hline
\end{tabular}
\end{table}

It is possible that there are tests with mistakes that was not reported. To calculate the probability of this happening the 6th tutorial test was manually checked for mistakes. Non were found. Thus it is assumed that tests that have mistakes in, but is not reported are unlikely and is not interoperated into the calculations.

In the 11 tutorials an average of 99.3\% of test is estimated to be graded correctly, as no corrections were made by students.  This result is seen to be brought down by the basic system having a lower accuracy rate. When only taking the complete grading system's result and average correctly grading tests is calculated to be 99.9\%.


\section{Deep Convolutional Neural Network results}
\label{sec:DCNNresult}

This section describes the results obtained on testing a trained neural network on a test dataset. Tests is conducted on 3 neural networks trained on different datasets and compared with each other. This testing proses is conducted to find the neural network weights that will classified had written digits the most accurately. The neural networks is tested on a test dataset generated by grading 900 student tests, while extracting the character images. The answers from these tests is used to create labels for each digit image. Thus each 28 by 28 pixel digit image has a accompanied label specifies what that digit is. The database contains 16 000 labelled images and was thus split into a training set of 11 000 digits and a test set of 5 000 digits. An additional dataset, called the MNIST dataset, \citep{mnist}, was also used in this proses. This dataset contains 60 000 training set digits and a test set of 10 000 digits. Each neural network was tested on both datasets. The results of these test is shown in the next section. Every network was trained for 8 hours on the same processor, before being tested.
\subsection{Trained on generated database}
In a first attempt at training the neural network the generated 11 000 digit training set was used to train the network.

\subsubsection{Accuracy of network}

The test accuracy of the neural network in on each dataset's test set is shown in Table \ref{tbl:nnResult1}.

\begin{table}
\caption{Test results for neural network trained on generated data.} \label{tbl:nnResult1}
  \centering
\begin{tabular}{|p{4cm}|p{5cm}|}
\hline
\textbf{Test dataset}&\textbf{Percentage accuracy}\\
\hline
MNIST dataset&94.62\%\\
\hline
Test generated dataset&92.16\%\\
\hline
\end{tabular}
\end{table}

\subsubsection{Conclusion on accuracy}

The results are promising, but an the average on the MNIST dataset is still to low. A standard digit classifier has an MNIST testing accuracy of above 99\%. A reason for this accuracy is attributed to the small training set size of the generated dataset. The deep neural network thus does not have enough data to accurately model each digit.

\subsection{Trained on MNIST database}
For the second neural network the MNIST dataset of 60 000 digit were used to train the network.

\subsubsection{Accuracy of network}

The test accuracy of the neural network in on each dataset's test set is shown in Table \ref{tbl:nnResult2}.

\begin{table}
\caption{Test results for neural network trained on MNIST dataset.} \label{tbl:nnResult2}
  \centering
\begin{tabular}{|p{4cm}|p{5cm}|}
\hline
\textbf{Test dataset}&\textbf{Percentage accuracy}\\
\hline
MNIST dataset&99.35\%\\
\hline
Test generated dataset&82.23\%\\
\hline
\end{tabular}
\end{table}

\subsubsection{Conclusion on accuracy}

The results is very poor in classifying the generated data. A main reason was found to be that the MNIST dataset having very preprocessed digits. In the test grader example digits are sometimes written to the side and the system cannot find a segment to recentre. This causes a few of centred digits that is not present in the MNIST dataset.

\subsection{Trained on mixed database}
For the final neural network a combined dataset was used. Thus the 11 000 training digits of the generated dataset and the 60 000 training digits of the MNIST was combined to train the model.

\subsubsection{Accuracy of network}

The test accuracy of the neural network in on each dataset's test set is shown in Table \ref{tbl:nnResult3}.

\begin{table}
\caption{Test results for neural network trained on generated data.} \label{tbl:nnResult3}
  \centering
\begin{tabular}{|p{4cm}|p{5cm}|}
\hline
\textbf{Test dataset}&\textbf{Percentage accuracy}\\
\hline
MNIST dataset&99.1\%\\
\hline
Test generated dataset&94.35\%\\
\hline
\end{tabular}
\end{table}

\subsubsection{Conclusion on accuracy}

The best result is thus achieved by combining the two datasets. This 94.35\% accuracy is high enough for the neural network to provide usefull information to the test grading system.