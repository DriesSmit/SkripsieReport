\chapter{Validation and results}
\label{ap:results}
\graphicspath{{Appendix5/Appendix5figures/}}

This appendix described additional test results obtained from experiments done on the automatic test grading system.

\section{All tutorial results}
\label{sec:tutorialResults}

\subsection{Overview}

The automatic test grader developed in this study was successfully used to grade 11 tutorial test in 2017. The number of students per tutorial varies due to students having valid excuses. On average 889 tests were written per tutorial. The results of these tutorials can be seen in Table \ref{tbl:tutResults} and Table \ref{tbl:tutResults2}.

\begin{table}
\caption{Description of tutorial results.} \label{tbl:tutResults}
  \centering
\begin{tabular}{|p{2cm}|p{4cm}|p{5cm}|}
\hline
\textbf{Tutorial number}&\textbf{Percentage tests graded correctly}&\textbf{Reason for results}\\
\hline
\multicolumn{3}{|l|}{The basic system is now implemented.}\\
\hline
Tutorial 1&98.4\% (14 mistakes)&The system has a problem with identifying crossed-out answers.\\
\hline
Tutorial 2&98.8\% (11 mistake)&The system still had a problem with crossed-out answers.  This problem was subsequently resolved.\\
\hline
Tutorial 3&99.4\% (5 mistakes)&The system made a few mistakes with answers with only character information.\\
\hline
Tutorial 4&98.5\% (13 mistakes)&A rounding error in the software led to some answers being marked incorrectly.\\
\hline
Tutorial 5&99.3\% (5 mistakes)&The system made a few mistakes with answers with only character information.\\
\hline
Tutorial 6&99.7\% (3 mistakes)&Again the system made a few mistakes with answers with only character information.\\
\hline
\end{tabular} 
\end{table}

\begin{table}
\caption{Description of tutorial results.} \label{tbl:tutResults2}
  \centering
\begin{tabular}{|p{2cm}|p{4cm}|p{5cm}|}
\hline
\textbf{Tutorial number}&\textbf{Number of tests graded incorrectly}&\textbf{Reason for results}\\
\hline
\multicolumn{3}{|l|}{The complete system,  with machine learning, is now implemented.}\\
\hline
Tutorial 7&99.9\% (1 mistake)&The system classified a crossed-out answer as being coloured in.\\
\hline
Tutorial 8&99.8\% (2 mistakes)&The system classified 2 crossed-out answers as being coloured in. This problem was subsequently resolved.\\
\hline
Tutorial 9&100.0\% (0 mistakes)&No mistakes where found.\\
\hline
Tutorial 10&99.9\% (1 mistakes)&This tutorial was discussed in Chapter \ref{ch:Results}. The student wrote over the negative sign bubble, confusing the system. This mistake is attributed to the student.\\
\hline
Tutorial 11&100.0\% (0 mistakes)&No mistakes where found.\\
\hline
\end{tabular}
\end{table}

It is possible that there are tests with mistakes that was not reported. To calculate the probability of this happening the 6th tutorial test was manually checked for mistakes. None were found. Thus it is assumed that tests that have mistakes in, but is not reported, are unlikely and is not incorporated into the calculations.

In the 11 tutorials an average of 99.3\% of tests are estimated to be graded correctly, as no corrections were made by students.  This percentage is lower that expected, because the basic system's averages are also included. When only taking the complete grading system's result, an average of correctly grading tests are calculated to be 99.9\%.


\section{Deep Convolutional Neural Network results}
\label{sec:DCNNresult}

This section describes the results obtained on testing a trained neural network on a test dataset. Tests are conducted on 3 neural networks trained on different datasets and compared with each other. This testing process is conducted to find the neural network weights that classifies written digits most accurately. The neural networks are tested on a test dataset generated by grading 900 student tests and extracting the character images. The answers from these tests was used to create labels for each digit image. Thus each 28 by 28 pixel digit image has an accompanied label specifying the digit. The dataset contains 16 000 labelled images and is split into a training set of 11 000 digits and a test set of 5 000 digits. An additional dataset, called the MNIST dataset, \citep{mnist}, is also used in this process. This dataset contains 60 000 training set digits and a test set of 10 000 digits. Each neural network is tested on both datasets. Every network was trained for 8 hours on the same processor, before being tested. The results of these test is shown in the next section.
\subsection{Trained on generated database}
In a first attempt at training a neural network the generated 11 000 digit training set was used to train the network.

\subsubsection{Accuracy of network}

The test accuracy of the neural network on both test sets are given in Table \ref{tbl:nnResult1}.

\begin{table}[h]
\caption{Test results for neural network trained on generated data.} \label{tbl:nnResult1}
  \centering
\begin{tabular}{|p{4cm}|p{5cm}|}
\hline
\textbf{Test dataset}&\textbf{Percentage accuracy}\\
\hline
MNIST dataset&94.62\%\\
\hline
Generated dataset&92.16\%\\
\hline
\end{tabular}
\end{table}

\subsubsection{Conclusion on accuracy}

The results are promising, but the average on the MNIST dataset is still too low. A standard digit classifier has a MNIST testing accuracy of above 99\%. A reason for this accuracy is attributed to the small training set size of the generated dataset. The deep neural network thus does not have enough data to accurately model each digit and starts to overfit on the data.

\subsection{Trained on MNIST database}
For the second neural network the MNIST dataset of 60 000 digit was used to train the network.

\subsubsection{Accuracy of network}
The test accuracy of the neural network on both test sets are given in Table \ref{tbl:nnResult2}.

\begin{table}[h]
\caption{Test results for neural network trained on MNIST dataset.} \label{tbl:nnResult2}
  \centering
\begin{tabular}{|p{4cm}|p{5cm}|}
\hline
\textbf{Test dataset}&\textbf{Percentage accuracy}\\
\hline
MNIST dataset&99.35\%\\
\hline
Generated dataset&83.23\%\\
\hline
\end{tabular}
\end{table}

\subsubsection{Conclusion on accuracy}

The resulting 83.23\% in classifying the generated data is low. The reason for this low accuracy is attributed to the MNIST dataset having only centred and normalized digits. In the test grader example digits are sometimes written off-centre and are unnormalized. The system mostly corrects for this, but there are cases where the digits cannot be centred. This causes a few off-centred digits that the neural network is not trained to classify.

\subsection{Trained on mixed database}
For the final neural network a combined dataset is used. The 11 000 training digits of the generated dataset and the 60 000 training digits of the MNIST is combined to train the model.

\subsubsection{Accuracy of network}
The test accuracy of the neural network on both test sets are given in Table \ref{tbl:nnResult3}.

\begin{table}
\caption{Test results for neural network trained on combined data.} \label{tbl:nnResult3}
  \centering
\begin{tabular}{|p{4cm}|p{5cm}|}
\hline
\textbf{Test dataset}&\textbf{Percentage accuracy}\\
\hline
MNIST dataset&99.1\%\\
\hline
Generated dataset&94.35\%\\
\hline
\end{tabular}
\end{table}

\subsubsection{Conclusion on accuracy}
The best result is thus achieved by combining the two datasets. This 94.35\% accuracy is high enough for the neural network to provide useful information to the test grading system. This neural network is used as the character recognition unit in the test grader.